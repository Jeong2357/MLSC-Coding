{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "342fc70e",
   "metadata": {},
   "source": [
    "# Commodity Trading with Regret\n",
    "\n",
    "### Background: Evans' Example 4.6.2\n",
    "In **Chapter 4, Example 2 (Commodity Trading)**, Evans introduces a model for optimal trading. The model relies on the assumption that \"the price of the wheat is known for the entire trading period\". This is extremely unrealistic. Nonetheless, this model provides a baseline, \"how well could have I performed had I known the future?\". It thus allows us to measure the regret of a given strategy.\n",
    "\n",
    "---\n",
    "\n",
    "### The Goal: Back-Testing with a Theoretical Ceiling\n",
    "We propose a **Regret Minimization** challenge.\n",
    "Suppose we trade a highly cyclical sector (e.g., **XLE**, the Energy Sector). Theoretically, a Machine Learning model should be able to learn the \"Buy Low, Sell High\" trends. By using the Pontryagin Maximum Principle (PMP), we can compute the **absolute best possible performance** for any given price history and use this to train our model.\n",
    "\n",
    "We measure our loss involving the regret factor:\n",
    "$$\\text{Regret} = (\\text{Theoretical Best via PMP}) - (\\text{Neural Network Performance})$$\n",
    "\n",
    "---\n",
    "\n",
    "### The Pipeline\n",
    "We will implement the following 5-step workflow:\n",
    "\n",
    "1.  **Extract Physics:** Download **XLE** (Energy ETF) data. Perform a cyclical curve fit to extract trend, frequency ($\\omega$), and amplitude ($A$) statistics.\n",
    "2.  **The Multiverse:** Generate \"Alternate Universes\" (synthetic sample paths) based on the statistics extracted in Step 1.\n",
    "3.  **The Oracle (PMP):** Compute the \"absolute best\" trading strategy for each alternate universe using Evans' PMP solution (since we know the full history in the simulation).\n",
    "4.  **The Student (ML):** Instantiate a Neural Network (e.g., LSTM) to learn the trading strategy. The model is trained to minimize the **regret** (imitate the Oracle).\n",
    "5.  **The Final Exam:** Back-test the trained model on **Real XLE Data** to see if it captured the underlying physics of the market.\n",
    "\n",
    "---\n",
    "\n",
    "**Who will come out on top?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bdf3fa",
   "metadata": {},
   "outputs": [],
   "source": "# Step 1: Imports and Data Extraction\n\nimport yfinance as yf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import curve_fit\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\nimport warnings, os, time\nwarnings.filterwarnings('ignore')\n\nSAVE_DIR = '/Users/yechanjeong/Desktop/stock'\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n# Configuration\nASSET_TICKER = 'XLE'\nHOLDING_COST = 0.0005   # Daily holding cost\nMAX_RATE = 5.0           # Max shares to buy/sell per day\nMAX_INV = 80.0           # Max inventory position\nWINDOW_SIZE = 30         # Lookback window for the Neural Network\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Running on {DEVICE}\")\n\n# --- A. Data Ingestion ---\nprint(\"Fetching Market Data...\")\ntrain_data_raw = yf.download(ASSET_TICKER, start='1999-01-01', end='2024-01-01', progress=False)\neval_data_raw = yf.download(ASSET_TICKER, start='2024-01-01', end='2026-02-20', progress=False)\n\n# Save for reproducibility\ntrain_data_raw.to_csv(os.path.join(SAVE_DIR, 'xle_train.csv'))\neval_data_raw.to_csv(os.path.join(SAVE_DIR, 'xle_eval.csv'))\n\n# Reload with consistent format\ntrain_data = pd.read_csv(os.path.join(SAVE_DIR, 'xle_train.csv'), header=[0,1], index_col=0, parse_dates=True)\neval_data = pd.read_csv(os.path.join(SAVE_DIR, 'xle_eval.csv'), header=[0,1], index_col=0, parse_dates=True)\n\ntrain_prices = train_data['Close'][ASSET_TICKER].values.astype(float)\neval_prices = eval_data['Close'][ASSET_TICKER].values.astype(float)\neval_dates = eval_data['Close'][ASSET_TICKER].index\ntrain_dates = train_data['Close'][ASSET_TICKER].index\n\nprint(f\"Training: {len(train_prices)} days, Eval: {len(eval_prices)} days\")\n\n# --- B. The Physics Fit ---\nlog_prices = np.log(train_prices)\nt_values = np.arange(len(log_prices))\n\ndef log_cyclical_model(t, drift, intercept, amp, omega, phase):\n    return drift * t + intercept + amp * np.sin(omega * t + phase)\n\nguess_omega = 2 * np.pi / (252 * 7)\np0 = [(log_prices[-1]-log_prices[0])/len(log_prices), log_prices[0], np.std(log_prices), guess_omega, 0]\nparams, _ = curve_fit(log_cyclical_model, t_values, log_prices, p0=p0)\ndrift, intercept, amp, omega, phase = params\n\nfitted_curve = log_cyclical_model(t_values, *params)\nresiduals = log_prices - fitted_curve\nnoise_std = np.std(residuals)\ndaily_innov_std = np.std(np.diff(residuals))\n\nprint(f\"\\n--- Market Physics Extracted ---\")\nprint(f\"Annual Drift: {(np.exp(drift*252)-1)*100:.2f}%\")\nprint(f\"Cycle Amplitude: {amp*100:.2f}% | Period: {(2*np.pi/omega)/252:.2f} Years\")\nprint(f\"Noise Volatility: {noise_std*100:.2f}%\")\n\n# Visualization\nfig, ax = plt.subplots(figsize=(12, 5))\nax.plot(train_dates, train_prices, label='XLE Training Data', alpha=0.6)\nax.plot(train_dates, np.exp(fitted_curve), 'r--', label='Fitted Geometric Cycle', linewidth=2)\nax.set_yscale('log')\nax.set_title(f\"{ASSET_TICKER} (1999-2023) vs. Fitted Geometric Cycle\")\nax.legend(); ax.grid(True, alpha=0.3); ax.set_xlabel('Date'); ax.set_ylabel('Price ($)')\nplt.tight_layout()\nplt.savefig(os.path.join(SAVE_DIR, 'fig1_physics_fit.png'), dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "code",
   "id": "o2myd4izt8",
   "source": "# Step 2: The Multiverse (Synthetic + Real Path Generation)\n\nCHUNK_LENGTH = 300  # ~1.2 years of trading days\nNUM_SYNTH = 200\nSTRIDE = 30  # Dense overlap for more real data coverage\n\ndef generate_synthetic_path(n_days):\n    \"\"\"Generate a synthetic price path with randomized physics parameters.\"\"\"\n    d = drift * np.random.uniform(0.3, 2.0)\n    a = amp * np.random.uniform(0.3, 3.0)\n    w = omega * np.random.uniform(0.5, 2.0)\n    ph = np.random.uniform(0, 2*np.pi)\n    sp = np.random.uniform(15, 150)\n    t = np.arange(n_days)\n    inn = np.random.normal(0, daily_innov_std * np.random.uniform(0.3, 2.0), n_days)\n    noise = np.cumsum(inn)\n    lp = np.log(sp) + d*t + a*np.sin(w*t+ph) + noise\n    lp = np.clip(lp, np.log(3), np.log(600))\n    return np.exp(lp)\n\nsynth_paths = [generate_synthetic_path(CHUNK_LENGTH) for _ in range(NUM_SYNTH)]\n\n# Sliding windows from REAL training data\nreal_chunks = []\nfor start in range(0, len(train_prices) - CHUNK_LENGTH, STRIDE):\n    real_chunks.append(train_prices[start:start+CHUNK_LENGTH])\n\n# Weight real data 2x by including duplicates\nall_paths = real_chunks + real_chunks + synth_paths\nn_real = len(real_chunks)\nprint(f\"Real chunks: {n_real} (x2={n_real*2}), Synthetic: {NUM_SYNTH}, Total: {len(all_paths)}\")\n\n# Visualization\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\nfor i in range(min(30, NUM_SYNTH)):\n    axes[0].plot(synth_paths[i], alpha=0.1, color='blue')\nfor i in range(min(20, n_real)):\n    axes[0].plot(real_chunks[i], alpha=0.2, color='red')\naxes[0].set_title('Synthetic (blue) & Real Chunks (red)')\naxes[0].set_xlabel('Day'); axes[0].set_ylabel('Price ($)'); axes[0].grid(True, alpha=0.3)\n\nreal_ret = np.diff(np.log(train_prices))\nsynth_ret = np.diff(np.log(synth_paths[0]))\naxes[1].hist(real_ret, bins=80, alpha=0.5, density=True, label='Real XLE')\naxes[1].hist(synth_ret, bins=80, alpha=0.5, density=True, label='Synthetic')\naxes[1].set_title('Return Distribution: Real vs Synthetic')\naxes[1].legend(); axes[1].grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(os.path.join(SAVE_DIR, 'fig2_synthetic_paths.png'), dpi=150, bbox_inches='tight')\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "okuh98rlzy",
   "source": "# Step 3: The Oracle (PMP - Pontryagin Maximum Principle)\n\ndef pmp_oracle_full(prices, max_rate=MAX_RATE, holding_cost=HOLDING_COST, max_inv=MAX_INV):\n    \"\"\"\n    Compute the PMP-optimal trading strategy for a known price path.\n    \n    Evans' Model: costate lambda(t) = lambda_0 + h*t\n    Bang-bang control: u(t) = max_rate * sign(lambda(t) - p(t))\n    Binary search on lambda_0 to satisfy terminal constraint x(T) ~ 0.\n    \n    Returns: pnl, actions, switching_function (oracle's unclipped intent)\n    \"\"\"\n    T = len(prices)\n    def simulate(lam0):\n        x = 0.0; cash = 0.0; acts = np.zeros(T)\n        for t in range(T):\n            sigma = lam0 + holding_cost*t - prices[t]\n            u = max_rate if sigma > 0 else -max_rate\n            if x + u > max_inv: u = max(max_inv - x, 0)\n            if x + u < -max_inv: u = min(-max_inv - x, 0)\n            acts[t] = u; cash -= prices[t]*u + holding_cost*abs(x); x += u\n        return x, cash + prices[-1]*x, acts\n\n    lo, hi = float(prices.min()-200), float(prices.max()+200)\n    for _ in range(200):\n        mid = (lo+hi)/2; fx, _, _ = simulate(mid)\n        if fx > 0.5: hi = mid\n        elif fx < -0.5: lo = mid\n        else: break\n    lam0 = (lo+hi)/2\n    _, pnl, acts = simulate(lam0)\n    # Switching function: oracle's INTENT before inventory clipping\n    sf = np.array([lam0 + holding_cost*t - prices[t] for t in range(T)])\n    return pnl, acts, sf\n\n# Compute oracle for all paths (cache duplicates)\nprint(f\"Computing PMP oracle for {len(all_paths)} paths...\")\nall_oracle_pnls, all_oracle_acts, all_oracle_sfs = [], [], []\n\nfor i, path in enumerate(all_paths):\n    if i >= n_real and i < 2*n_real:  # Second copy of real chunks - reuse\n        orig_i = i - n_real\n        all_oracle_pnls.append(all_oracle_pnls[orig_i])\n        all_oracle_acts.append(all_oracle_acts[orig_i])\n        all_oracle_sfs.append(all_oracle_sfs[orig_i])\n    else:\n        pnl, acts, sf = pmp_oracle_full(path)\n        all_oracle_pnls.append(pnl); all_oracle_acts.append(acts); all_oracle_sfs.append(sf)\n    if (i+1) % 100 == 0: print(f\"  {i+1}/{len(all_paths)}\")\n\noracle_pnls = np.array(all_oracle_pnls)\nprint(f\"Oracle PnL: mean=${np.mean(oracle_pnls):.0f}\")\n\n# Visualization: Oracle on a real data chunk\nfig, axes = plt.subplots(4, 1, figsize=(14, 12), sharex=True)\nidx = n_real // 2\naxes[0].plot(all_paths[idx], 'b-')\naxes[0].set_title(f'Real Chunk #{idx} (Oracle PnL=${oracle_pnls[idx]:.0f})')\naxes[0].set_ylabel('Price ($)'); axes[0].grid(True, alpha=0.3)\n\nsf = all_oracle_sfs[idx]\naxes[1].plot(sf, 'purple', alpha=0.7)\naxes[1].fill_between(range(len(sf)), sf, alpha=0.2, color='purple')\naxes[1].axhline(y=0, color='k', linestyle='--', alpha=0.5)\naxes[1].set_title('Switching Function sigma(t) = lambda_0 + h*t - p(t)  [>0 = buy intent, <0 = sell]')\naxes[1].set_ylabel('sigma(t)'); axes[1].grid(True, alpha=0.3)\n\naxes[2].bar(range(CHUNK_LENGTH), all_oracle_acts[idx],\n            color=['green' if a>0 else 'red' for a in all_oracle_acts[idx]], alpha=0.6, width=1)\naxes[2].set_title('Oracle Actions (inventory-clipped bang-bang)')\naxes[2].set_ylabel('Action (u)'); axes[2].grid(True, alpha=0.3)\n\ninv = np.cumsum(all_oracle_acts[idx])\naxes[3].fill_between(range(CHUNK_LENGTH), inv, alpha=0.3, color='orange')\naxes[3].plot(inv, 'orange'); axes[3].set_title('Oracle Inventory')\naxes[3].set_ylabel('Inventory'); axes[3].set_xlabel('Trading Day'); axes[3].grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(os.path.join(SAVE_DIR, 'fig3_oracle_example.png'), dpi=150, bbox_inches='tight')\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "9mb92b53xz",
   "source": "# Step 4: The Student (GRU Ensemble Neural Network)\n\nclass TradingGRU(nn.Module):\n    \"\"\"2-layer GRU with MLP head, outputs trading signal in [-1, 1].\"\"\"\n    def __init__(self, input_size=5, hidden_size=96, num_layers=2, dropout=0.15):\n        super().__init__()\n        self.gru = nn.GRU(input_size, hidden_size, num_layers,\n                         batch_first=True, dropout=dropout)\n        self.head = nn.Sequential(\n            nn.Linear(hidden_size, 48), nn.ReLU(), nn.Dropout(0.1),\n            nn.Linear(48, 1), nn.Tanh()\n        )\n    def forward(self, x):\n        out, _ = self.gru(x)\n        return self.head(out[:, -1, :])\n\ndef create_features(prices, window_size):\n    \"\"\"Create 5 features per timestep: z-score, returns, vol, mean-rev, momentum.\"\"\"\n    n = len(prices)\n    log_p = np.log(np.clip(prices, 1e-6, None)).astype(np.float32)\n    returns = np.zeros(n, dtype=np.float32); returns[1:] = np.diff(log_p)\n    sma = np.zeros(n, dtype=np.float32)\n    for i in range(n): sma[i] = np.mean(log_p[max(0,i-50):i+1])\n    vol = np.zeros(n, dtype=np.float32)\n    for i in range(1, n): vol[i] = np.std(returns[max(0,i-20):i+1])\n    vol[0] = vol[1] if n > 1 else 0.01\n\n    features_list = []\n    for i in range(window_size, n):\n        w = log_p[i-window_size:i]; p_std = max(np.std(w), 1e-6)\n        norm_p = (w - np.mean(w)) / p_std\n        ret_std = max(np.std(returns[max(0,i-100):i+1]), 1e-6)\n        w_ret = returns[i-window_size:i] / ret_std\n        vol_mean = max(np.mean(vol[max(0,i-100):i+1]), 1e-6)\n        w_vol = vol[i-window_size:i] / vol_mean\n        w_mr = (log_p[i-window_size:i] - sma[i-window_size:i]) / p_std\n        w_mom = np.zeros(window_size, dtype=np.float32)\n        for j in range(window_size):\n            ti = i - window_size + j; lb = min(20, ti)\n            w_mom[j] = (log_p[ti] - log_p[ti-lb]) / p_std if lb > 0 else 0\n        features_list.append(np.stack([norm_p, w_ret, w_vol, w_mr, w_mom], axis=-1).astype(np.float32))\n    result = np.array(features_list, dtype=np.float32)\n    return np.clip(np.nan_to_num(result, nan=0.0, posinf=5.0, neginf=-5.0), -10.0, 10.0)\n\ndef create_training_data():\n    \"\"\"Target: tanh of normalized switching function (oracle's intent).\"\"\"\n    all_X, all_y = [], []\n    for i in range(len(all_paths)):\n        feats = create_features(all_paths[i], WINDOW_SIZE)\n        sf = all_oracle_sfs[i][WINDOW_SIZE:]\n        price_scale = np.std(all_paths[i]) * 0.5\n        target = np.tanh(sf / max(price_scale, 1.0)).astype(np.float32)\n        ml = min(len(feats), len(target))\n        all_X.append(feats[:ml]); all_y.append(target[:ml])\n    X = np.concatenate(all_X); y = np.concatenate(all_y)\n    mask = np.isfinite(X).all(axis=(1,2)) & np.isfinite(y)\n    return X[mask], y[mask]\n\nprint(\"Building training data from switching function targets...\")\nX_train, y_train = create_training_data()\nprint(f\"Samples: {len(X_train)}, target: mean={y_train.mean():.3f}, std={y_train.std():.3f}\")\nprint(f\"Positive (buy): {(y_train>0).mean()*100:.1f}%, Negative (sell): {(y_train<0).mean()*100:.1f}%\")\n\nX_tensor = torch.FloatTensor(X_train)\ny_tensor = torch.FloatTensor(y_train).reshape(-1, 1)\ndataset = TensorDataset(X_tensor, y_tensor)\n\n# --- Train Ensemble of 3 GRU Models ---\nNUM_MODELS = 3\nNUM_EPOCHS = 80\nmodels = []\n\nfor m_idx in range(NUM_MODELS):\n    print(f\"\\n--- Training model {m_idx+1}/{NUM_MODELS} ---\")\n    torch.manual_seed(42 + m_idx * 100)\n    dataloader = DataLoader(dataset, batch_size=512, shuffle=True)\n    model = TradingGRU(input_size=5, hidden_size=96, num_layers=2, dropout=0.15)\n    optimizer = optim.Adam(model.parameters(), lr=0.002)\n    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=20, T_mult=2, eta_min=1e-5)\n    best_loss = float('inf'); losses = []\n\n    for epoch in range(NUM_EPOCHS):\n        model.train(); el = 0.0; nb = 0\n        for xb, yb in dataloader:\n            optimizer.zero_grad(); pred = model(xb)\n            loss = nn.MSELoss()(pred, yb)\n            if torch.isnan(loss): continue\n            loss.backward(); torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step(); el += loss.item(); nb += 1\n        scheduler.step(); avg = el / max(nb, 1); losses.append(avg)\n        if avg < best_loss:\n            best_loss = avg\n            best_state = {k: v.clone() for k, v in model.state_dict().items()}\n        if (epoch+1) % 20 == 0:\n            model.eval()\n            with torch.no_grad():\n                sp = model(X_tensor[:5000]).squeeze()\n                acc = ((sp * y_tensor[:5000].squeeze()) > 0).float().mean()\n            model.train()\n            print(f\"  Epoch {epoch+1}, Loss: {avg:.6f}, Dir Acc: {acc:.3f}\")\n\n    model.load_state_dict(best_state); models.append(model)\n    print(f\"  Best loss: {best_loss:.6f}\")\n\n# Ensemble accuracy\nfor m in models: m.eval()\nwith torch.no_grad():\n    preds = torch.stack([m(X_tensor[:5000]) for m in models]).mean(dim=0).squeeze()\n    acc = ((preds * y_tensor[:5000].squeeze()) > 0).float().mean()\nprint(f\"\\nEnsemble directional accuracy: {acc:.3f}\")\n\nfig, ax = plt.subplots(figsize=(10, 4))\nax.plot(losses, 'b-', linewidth=2)\nax.set_title('Training Loss (MSE on Switching Function)'); ax.set_xlabel('Epoch'); ax.set_ylabel('Loss')\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(os.path.join(SAVE_DIR, 'fig4_training_loss.png'), dpi=150, bbox_inches='tight')\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "vtpeorxdvx",
   "source": "# Step 5: The Final Exam (Back-testing on Real 2024-2025 Data)\n\ndef backtest_ensemble(models, prices, window_size, max_rate, holding_cost, max_inv=MAX_INV):\n    \"\"\"Run ensemble model on price series with terminal constraint x(T)=0.\"\"\"\n    for m in models: m.eval()\n    features = create_features(prices, window_size)\n    n = len(prices); actions = np.zeros(n); inventory = np.zeros(n)\n    x = 0.0; cash = 0.0\n\n    # Reserve last ceil(max_inv/max_rate) days for forced liquidation (same as oracle's x(T)=0)\n    liquidation_days = int(np.ceil(max_inv / max_rate))\n    last_nn_day = len(features) - liquidation_days\n\n    with torch.no_grad():\n        for i in range(len(features)):\n            t = i + window_size\n            days_left = len(features) - i\n\n            if i < last_nn_day:\n                # Normal NN trading\n                feat = torch.FloatTensor(features[i:i+1])\n                raw = np.mean([m(feat).item() for m in models])\n                u = np.sign(raw) * max_rate * min(abs(raw) * 1.5, 1.0)\n            else:\n                # Forced liquidation: unwind inventory to reach x(T)=0\n                if abs(x) < 1e-6:\n                    u = 0.0\n                else:\n                    u_needed = -x / max(days_left, 1)\n                    u = np.clip(u_needed, -max_rate, max_rate)\n\n            if x + u > max_inv: u = max(max_inv - x, 0)\n            if x + u < -max_inv: u = min(-max_inv - x, 0)\n            actions[t] = u; cash -= prices[t]*u + holding_cost*abs(x); x += u; inventory[t] = x\n\n    cash += prices[-1] * x  # residual (should be ~0)\n    return cash, actions, inventory\n\ndef pmp_oracle_simple(prices, max_rate=MAX_RATE, holding_cost=HOLDING_COST, max_inv=MAX_INV):\n    \"\"\"PMP oracle for evaluation (returns pnl, actions).\"\"\"\n    T = len(prices)\n    def simulate(lam0):\n        x=0.0; cash=0.0; acts=np.zeros(T)\n        for t in range(T):\n            sigma = lam0 + holding_cost*t - prices[t]\n            u = max_rate if sigma > 0 else -max_rate\n            if x+u > max_inv: u = max(max_inv-x, 0)\n            if x+u < -max_inv: u = min(-max_inv-x, 0)\n            acts[t]=u; cash -= prices[t]*u + holding_cost*abs(x); x+=u\n        return x, cash+prices[-1]*x, acts\n    lo, hi = float(prices.min()-200), float(prices.max()+200)\n    for _ in range(200):\n        mid=(lo+hi)/2; fx,_,_=simulate(mid)\n        if fx>0.5: hi=mid\n        elif fx<-0.5: lo=mid\n        else: break\n    _,pnl,acts = simulate((lo+hi)/2)\n    return pnl, acts\n\ndef compute_cum_pnl(prices, actions, holding_cost):\n    cum = np.zeros(len(prices)); x=0.0; cash=0.0\n    for t in range(len(prices)):\n        cash -= prices[t]*actions[t] + holding_cost*abs(x)\n        x += actions[t]; cum[t] = cash + x*prices[t]\n    return cum\n\n# --- Run the back-test ---\nprint(f\"Back-testing on {len(eval_prices)} days of real XLE data...\")\nprint(f\"(Both Oracle and NN enforce terminal constraint x(T)=0)\")\nnn_pnl, nn_actions, nn_inv = backtest_ensemble(models, eval_prices, WINDOW_SIZE, MAX_RATE, HOLDING_COST)\noracle_pnl, oracle_acts = pmp_oracle_simple(eval_prices)\noracle_inv = np.cumsum(oracle_acts)\nbh_pnl = eval_prices[-1] - eval_prices[0]\n\nnn_cum = compute_cum_pnl(eval_prices, nn_actions, HOLDING_COST)\noracle_cum = compute_cum_pnl(eval_prices, oracle_acts, HOLDING_COST)\nbh_cum = eval_prices - eval_prices[0]\n\nregret = oracle_pnl - nn_pnl\nratio = nn_pnl / oracle_pnl * 100 if oracle_pnl > 0 else 0\n\nprint(f\"\\n{'='*55}\")\nprint(f\"  EVALUATION RESULTS (2024-2025)\")\nprint(f\"{'='*55}\")\nprint(f\"  PMP Oracle (Theoretical Best): ${oracle_pnl:>12.2f}\")\nprint(f\"  Neural Network (Ensemble):     ${nn_pnl:>12.2f}\")\nprint(f\"  Buy-and-Hold Benchmark:        ${bh_pnl:>12.2f}\")\nprint(f\"{'='*55}\")\nprint(f\"  Regret (Oracle - NN):          ${regret:>12.2f}\")\nprint(f\"  NN captures {ratio:.1f}% of Oracle\")\nprint(f\"  NN vs Buy-Hold:                ${nn_pnl - bh_pnl:>12.2f}\")\nprint(f\"{'='*55}\")\n\n# In-sample sanity check\nis_prices = train_prices[-504:]\nnn_is, _, _ = backtest_ensemble(models, is_prices, WINDOW_SIZE, MAX_RATE, HOLDING_COST)\nor_is, _ = pmp_oracle_simple(is_prices)\nprint(f\"\\nIn-sample (last 2yr): NN=${nn_is:.2f} ({nn_is/or_is*100:.1f}% of Oracle=${or_is:.2f})\")\n\n# --- FIGURES ---\n# Figure 5: Full evaluation\nfig, axes = plt.subplots(4, 1, figsize=(14, 16))\naxes[0].plot(eval_dates, eval_prices, 'k-', lw=1.5, label='XLE Price')\naxes[0].set_title('XLE Evaluation Period (2024-2025)', fontsize=14)\naxes[0].set_ylabel('Price ($)'); axes[0].legend(); axes[0].grid(True, alpha=0.3)\n\naxes[1].plot(eval_dates, nn_cum, 'b-', lw=2, label=f'NN Ensemble (${nn_pnl:.2f})')\naxes[1].plot(eval_dates, oracle_cum, 'r--', lw=2, label=f'PMP Oracle (${oracle_pnl:.2f})')\naxes[1].plot(eval_dates, bh_cum, 'g:', lw=2, label=f'Buy & Hold (${bh_pnl:.2f})')\naxes[1].set_title('Cumulative P&L Comparison', fontsize=14)\naxes[1].set_ylabel('P&L ($)'); axes[1].legend(fontsize=11); axes[1].grid(True, alpha=0.3)\naxes[1].axhline(y=0, color='k', alpha=0.2)\n\naxes[2].bar(range(len(nn_actions)), nn_actions,\n            color=['green' if a>0 else ('red' if a<0 else 'gray') for a in nn_actions], alpha=0.5, width=1)\naxes[2].set_title('NN Trading Actions', fontsize=14); axes[2].set_ylabel('Action (u)'); axes[2].grid(True, alpha=0.3)\n\naxes[3].fill_between(range(len(nn_inv)), nn_inv, alpha=0.3, color='blue')\naxes[3].plot(nn_inv, 'b-', lw=1)\naxes[3].set_title('NN Inventory Position', fontsize=14); axes[3].set_ylabel('Inventory')\naxes[3].set_xlabel('Trading Day'); axes[3].grid(True, alpha=0.3)\naxes[3].axhline(y=0, color='k', ls='--', alpha=0.3)\nplt.tight_layout()\nplt.savefig(os.path.join(SAVE_DIR, 'fig5_evaluation_backtest.png'), dpi=150, bbox_inches='tight')\nplt.show()\n\n# Figure 6: Oracle vs NN comparison\nfig, axes = plt.subplots(2, 2, figsize=(16, 10))\naxes[0,0].plot(eval_dates, oracle_acts, 'r-', alpha=0.6)\naxes[0,0].set_title('Oracle Actions', fontsize=13); axes[0,0].set_ylabel('Action'); axes[0,0].grid(True, alpha=0.3)\naxes[0,1].plot(eval_dates, nn_actions, 'b-', alpha=0.6)\naxes[0,1].set_title('NN Actions', fontsize=13); axes[0,1].set_ylabel('Action'); axes[0,1].grid(True, alpha=0.3)\naxes[1,0].fill_between(eval_dates, oracle_inv, alpha=0.2, color='red')\naxes[1,0].plot(eval_dates, oracle_inv, 'r-', alpha=0.7)\naxes[1,0].set_title('Oracle Inventory'); axes[1,0].set_ylabel('Inventory'); axes[1,0].grid(True, alpha=0.3)\naxes[1,1].fill_between(eval_dates, nn_inv, alpha=0.2, color='blue')\naxes[1,1].plot(eval_dates, nn_inv, 'b-', alpha=0.7)\naxes[1,1].set_title('NN Inventory'); axes[1,1].set_ylabel('Inventory'); axes[1,1].grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(os.path.join(SAVE_DIR, 'fig6_actions_comparison.png'), dpi=150, bbox_inches='tight')\nplt.show()\n\n# Figure 7: Performance bar chart\nfig, ax = plt.subplots(figsize=(9, 6))\nstrats = ['PMP Oracle\\n(Theoretical Best)', 'NN Ensemble\\n(Our Model)', 'Buy & Hold\\n(Benchmark)']\npvals = [oracle_pnl, nn_pnl, bh_pnl]\nbars = ax.bar(strats, pvals, color=['gold', 'steelblue', 'lightcoral'], edgecolor='black', lw=1.2)\nmx = max(abs(p) for p in pvals) if any(p!=0 for p in pvals) else 1\nfor b, p in zip(bars, pvals):\n    off = mx*0.03*(1 if p>=0 else -1)\n    ax.text(b.get_x()+b.get_width()/2., b.get_height()+off,\n            f'${p:.2f}', ha='center', va='bottom' if p>=0 else 'top', fontsize=12, fontweight='bold')\nax.set_title('Strategy Performance Comparison (2024-2025)', fontsize=14)\nax.set_ylabel('Total P&L ($)'); ax.grid(True, alpha=0.3, axis='y')\nax.axhline(y=0, color='k', alpha=0.3)\nplt.tight_layout()\nplt.savefig(os.path.join(SAVE_DIR, 'fig7_performance_summary.png'), dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"\\nAll figures saved to:\", SAVE_DIR)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}